---
title: "Bank Marketing Case Study 1"
author: "Dominica Peri"
date: "2025-09-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Executive Summary (Last)

2. The Problem
    1. Introduction/Background
    2. Purpose of study/importance of study/statement of problem
    3. Questions to be answered/conceptual statement of hypotheses
    4. Outline of remainder of report (brief)
    
3. Review of Related Literature
    1. Acquaint reader with existing methodologies used in this area
    
4. Methodology
    1. Identification, classification and operationalization of variables.
    2. Statements of hypotheses being tested and/or models being developed.
    3. Sampling techniques, if full data is not being used.
    4. Data collection process, including data sources, data size, etc. Primary/secondary?
    5. Modeling analysis/techniques used
    6. Methodological assumptions and limitations.
    
5. Data
    1. Data cleaning
    2. Data preprocessing
    3. Data limitations
    
6. Findings (Results)
    1. Results presented in tables or charts when appropriate
    2. Results reported with respect to hypotheses/models.
    3. Factual information kept separate from interpretation, inference and evaluation.
    
7. Conclusions and Recommendations
    1. Discuss alternative methodologies

#____________________________________________________________________________

### Load Libraries

```{r Libraries}
library(MASS) 
library(ggplot2)
library(tidyverse)
library(corrplot)
library(car)
library(GGally)
library(ROCR)
library(data.table)
library(Hmisc)
library(tibble)
library(knitr)
library(here)
library(ROCR)
```

### Import Data

```{r}
data_path <- "https://raw.githubusercontent.com/kccasey1985/Case-Study-Bank-Marketting-/main/bank-additional-full.csv"

df <- fread(data_path, sep = ";") |> as_tibble()
glimpse(df)
```

```{r }
str(df)
```

```{r}
summary(df)
```

### Data Cleaning

```{r }
df2 <- df |>
  mutate(
    job         = factor(job),
    marital     = factor(marital),
    education   = factor(education),
    default     = factor(default),
    loan        = factor(loan),
    contact     = factor(contact),
    month       = factor(month),
    day_of_week = factor(day_of_week),
    poutcome    = factor(poutcome),
    y           = factor(y),       
    housing     = factor(housing)
  )
```

```{r }
str(df2)
```

```{r }
summary(df2)
```

```{r }
anyNA(df2)
```

```{r }
df2 <- df2 |>
  mutate(
    y = factor(y, levels = c("no","yes")),
    age = cut(age,
              breaks = c(20, 30, 40, 50, 60, 70, 100),
              labels = c("21-30", "31-40", "41-50", "51-60", "61-70", "71+"),
              right = FALSE)
  ) |>
  select(-duration)
```

```{r }
summary(df2)
```

### Correlation Matrix

```{r }
df2_num <- dplyr::select_if(df2, is.numeric)
if (ncol(df2_num) > 1) {
  matrix <- cor(df2_num)
  corrplot(matrix, method = "number")
}
```

```{r}
# --- Remove highly correlated macro vars (as you did before) ---
# These are common to drop for this dataset due to multicollinearity
drop_cols <- c("emp.var.rate","cons.price.idx","euribor3m","nr.employed")
drop_cols <- intersect(drop_cols, names(df2))
df3 <- subset(df2, select = setdiff(names(df2), drop_cols))
```


### Models

#### Logistic Regression

```{r}
logit_model <- glm(y ~ ., data = df2, family = binomial)
summary(logit_model)
```

#### LDA

```{r}
df_clean <- df3[complete.cases(df3), ]
lda_model <- lda(y ~ ., data = df_clean)
summary(lda_model)
```

### Predictions / AUC

#### Logistic Regression

```{r}
logit_probs <- predict(logit_model, newdata = df_clean, type = "response")
roc_logit <- pROC::roc(df_clean$y, logit_probs, levels = c("no","yes"))
logit_auc <- as.numeric(roc_logit$auc)
```

#### LDA

```{r}
lda_probs <- predict(lda_model, newdata = df_clean)$posterior[, "yes"]
roc_lda <- pROC::roc(df_clean$y, lda_probs, levels = c("no","yes"))
lda_auc <- as.numeric(roc_lda$auc)
```

### Comparison Table

```{r}
model_summary <- data.frame(
  Model = c("Logistic Regression", "LDA"),
  AUC = c(round(logit_auc, 4), round(lda_auc, 4))
)
knitr::kable(model_summary, caption = "Model Performance Summary (All df3, Unbalanced)")
```

### Balance the Data

```{r}
# Train/Test Split

# Balanced dataset
df3_yes <- dplyr::filter(df3, y == "yes")
df3_no  <- dplyr::filter(df3, y == "no")
df3_no_random <- sample_n(df3_no, nrow(df3_yes))
df3_bal <- rbind(df3_yes, df3_no_random)

# Unbalanced
train_index <- createDataPartition(df3$y, p = 0.7, list = FALSE)
train_unbal <- df3[train_index, ]
test_unbal  <- df3[-train_index, ]

# Balanced
train_index_bal <- createDataPartition(df3_bal$y, p = 0.7, list = FALSE)
train_bal <- df3_bal[train_index_bal, ]
test_bal  <- df3_bal[-train_index_bal, ]

formula <- y ~ age + job + marital + education + default + housing + loan +
  contact + month + day_of_week + poutcome + campaign + pdays + previous +
  cons.conf.idx

# Model fit
fit_models <- function(train_df) {
  list(
    lr   = glm(formula, data = train_df, family = binomial),
    tree = rpart(formula, data = train_df, method = "class",
                 control = rpart.control(cp = 0.001, minbucket = 50)),
    rf   = randomForest(form, data = train_df,
                        ntree = 300,
                        mtry  = floor(sqrt(length(all.vars(form)) - 1)),
                        nodesize = 25,
                        importance = TRUE)
  )
}

# Predict Probabilities
predict_probs <- function(models, newdata) {
  tibble(
    prob_lr   = as.numeric(predict(models$lr,   newdata = newdata, type = "response")),
    prob_tree = as.numeric(predict(models$tree, newdata = newdata, type = "prob")[, "yes"]),
    prob_rf   = as.numeric(predict(models$rf,   newdata = newdata, type = "prob")[, "yes"])
  )
}

# Evaluate
evaluate_probs <- function(actual, probs_tbl, threshold = 0.5, label = "") {
  stopifnot(is.factor(actual), all(levels(actual) == c("no","yes")))
  name_map <- c(prob_lr = "Logistic Regression", prob_tree = "Decision Tree", prob_rf = "Random Forest")
  purrr::map2_dfr(names(probs_tbl), probs_tbl, function(name, p) {
    pred <- factor(ifelse(p > threshold, "yes", "no"), levels = c("no","yes"))
    roc_obj <- pROC::roc(response = actual, predictor = p, levels = c("no","yes"))
    cm <- caret::confusionMatrix(pred, actual, positive = "yes")
    tibble(
      Set         = label,
      Model       = name_map[[name]],
      AUC         = as.numeric(roc_obj$auc),
      Accuracy    = as.numeric(cm$overall["Accuracy"]),
      Sensitivity = as.numeric(cm$byClass["Sensitivity"]),
      Specificity = as.numeric(cm$byClass["Specificity"])
    )
  })
}

models_unbal <- fit_models(train_unbal)
models_bal   <- fit_models(train_bal)

probs_unbal_on_unbal <- predict_probs(models_unbal, test_unbal)
probs_bal_on_unbal   <- predict_probs(models_bal,   test_unbal)

probs_unbal_on_bal   <- predict_probs(models_unbal, test_bal)
probs_bal_on_bal     <- predict_probs(models_bal,   test_bal)

# Evaluate (AUC, Accuracy, Sensitivity, Specificity)
eval_unbal_on_unbal <- evaluate_probs(test_unbal$y, probs_unbal_on_unbal, 0.5, "UnbalTrain → UnbalTest")
eval_bal_on_unbal   <- evaluate_probs(test_unbal$y, probs_bal_on_unbal,   0.5, "BalTrain → UnbalTest")

eval_unbal_on_bal   <- evaluate_probs(test_bal$y,   probs_unbal_on_bal,   0.5, "UnbalTrain → BalTest")
eval_bal_on_bal     <- evaluate_probs(test_bal$y,   probs_bal_on_bal,     0.5, "BalTrain → BalTest")

perf_all <- bind_rows(
  eval_unbal_on_unbal,
  eval_bal_on_unbal,
  eval_unbal_on_bal,
  eval_bal_on_bal
)

knitr::kable(perf_all %>% arrange(desc(AUC)),
             digits = 4,
             caption = "Mirrored Performance: Unbalanced vs Balanced (Train/Test)")

```

