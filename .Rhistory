df <- fread(here("bank-additional-full.csv"), sep = ";") |> as_tibble()
glimpse(df)
head(df)
str(df)
summary(df)
df2 <- df |>
mutate(job = factor(job),
marital  = factor(marital),
education  = factor(education),
default  = factor(default),
loan  = factor(loan),
contact  = factor(contact),
month   = factor(month),
day_of_week  = factor(day_of_week),
poutcome    = factor(poutcome),
y   = factor(y),
housing  = factor(housing))
str(df2)
summary(df2)
anyNA(df2)
df2 <- df2 |>
mutate(age= cut(age, breaks = c(20, 30, 40, 50, 60, 70, 100),
labels = c("21-30", "31-40", "41-50", "51-60", "61-70", "71+"),
right = F))|>
select(-duration)
summary(df2)
df2_num <- dplyr::select_if(df2, is.numeric)
matrix = cor(df2_num)
corrplot(matrix, method = c("number"))
corrplot::corrplot(matrix, method = "circle")
lm_model <- lm(y ~ ., data = df2)
# Make sure y is a factor with two levels
df2$y <- factor(df2$y, levels = c("no", "yes"))
# Logistic regression
logit_model <- glm(y ~ ., data = df2, family = binomial)
summary(logit_model)
alias(lm_model)
vif_values <- car::vif(lm_model)
#LDA
# Train LDA model
lda_model <- lda(y ~ ., data = df2)
# Get predictions
lda_pred <- predict(lda_model)
# View predicted classes
head(lda_pred$class)
library(MASS)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(car)
library(GGally)
library(ROCR)
library(data.table)
library(Hmisc)
library(tibble)
library(knitr)
library(here)
library(ROCR)
library(ROCR)
# Predict probabilities from logistic model
logit_probs <- predict(logit_model, type = "response")
# ROC and AUC for logistic regression
logit_pred <- prediction(logit_probs, df2$y)
# Remove rows with missing values used in the model
df_clean <- df2[complete.cases(df2), ]
# Predict again
logit_probs <- predict(logit_model, newdata = df_clean, type = "response")
# Create ROCR objects
library(ROCR)
logit_pred <- prediction(logit_probs, df_clean$y)
logit_perf <- performance(logit_pred, "tpr", "fpr")
# Plot ROC
plot(logit_perf, col = "blue", main = "ROC Curve: Logistic vs LDA")
abline(a=0, b=1, lty=2, col="gray")
# AUC
logit_auc <- performance(logit_pred, "auc")@y.values[[1]]
print(paste("Logistic Regression AUC:", round(logit_auc, 4)))
lda_pred <- predict(lda_model, newdata = df_clean)
lda_probs <- lda_pred$posterior[,2]
lda_roc <- prediction(lda_probs, df_clean$y)
lda_perf <- performance(lda_roc, "tpr", "fpr")
plot(lda_perf, col = "red", add = TRUE)
library(ROCR)
# Filter clean data (no missing rows)
df_clean <- df2[complete.cases(df2), ]
# Logistic model predictions
logit_probs <- predict(logit_model, newdata = df_clean, type = "response")
logit_pred <- prediction(logit_probs, df_clean$y)
logit_perf <- performance(logit_pred, "tpr", "fpr")
# Plot logistic ROC curve FIRST
plot(logit_perf, col = "blue", main = "ROC Curve: Logistic vs LDA", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
# Logistic AUC
logit_auc <- performance(logit_pred, "auc")@y.values[[1]]
print(paste("Logistic Regression AUC:", round(logit_auc, 4)))
# LDA predictions
lda_pred <- predict(lda_model, newdata = df_clean)
lda_probs <- lda_pred$posterior[, 2]
lda_roc <- prediction(lda_probs, df_clean$y)
lda_perf <- performance(lda_roc, "tpr", "fpr")
# Overlay LDA ROC curve
plot(lda_perf, col = "red", add = TRUE, lwd = 2)
# LDA AUC
lda_auc <- performance(lda_roc, "auc")@y.values[[1]]
print(paste("LDA AUC:", round(lda_auc, 4)))
# Legend
legend("bottomright",
legend = c(paste("Logistic AUC:", round(logit_auc, 4)),
paste("LDA AUC:", round(lda_auc, 4))),
col = c("blue", "red"), lwd = 2)
# Predict probabilities from logistic model
logit_probs <- predict(logit_model, type = "response")
# ROC and AUC for logistic regression
logit_pred <- prediction(logit_probs, df2$y)
# Remove rows with missing values used in the model
df_clean <- df2[complete.cases(df2), ]
# Predict again
logit_probs <- predict(logit_model, newdata = df_clean, type = "response")
# Create ROCR objects
logit_pred <- prediction(logit_probs, df_clean$y)
logit_perf <- performance(logit_pred, "tpr", "fpr")
# Plot ROC
plot(logit_perf, col = "blue", main = "ROC Curve: Logistic vs LDA")
abline(a=0, b=1, lty=2, col="gray")
# AUC
logit_auc <- performance(logit_pred, "auc")@y.values[[1]]
print(paste("Logistic Regression AUC:", round(logit_auc, 4)))
lda_pred <- predict(lda_model, newdata = df_clean)
lda_probs <- lda_pred$posterior[,2]
lda_roc <- prediction(lda_probs, df_clean$y)
lda_perf <- performance(lda_roc, "tpr", "fpr")
plot(lda_perf, col = "red", add = TRUE)
# Filter clean data (no missing rows)
df_clean <- df2[complete.cases(df2), ]
# Logistic model predictions
logit_probs <- predict(logit_model, newdata = df_clean, type = "response")
logit_pred <- prediction(logit_probs, df_clean$y)
logit_perf <- performance(logit_pred, "tpr", "fpr")
# Plot logistic ROC curve FIRST
plot(logit_perf, col = "blue", main = "ROC Curve: Logistic vs LDA", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
# Logistic AUC
logit_auc <- performance(logit_pred, "auc")@y.values[[1]]
print(paste("Logistic Regression AUC:", round(logit_auc, 4)))
# LDA predictions
lda_pred <- predict(lda_model, newdata = df_clean)
lda_probs <- lda_pred$posterior[, 2]
lda_roc <- prediction(lda_probs, df_clean$y)
lda_perf <- performance(lda_roc, "tpr", "fpr")
# Overlay LDA ROC curve
plot(lda_perf, col = "red", add = TRUE, lwd = 2)
# LDA AUC
lda_auc <- performance(lda_roc, "auc")@y.values[[1]]
print(paste("LDA AUC:", round(lda_auc, 4)))
# Legend
legend("bottomright",
legend = c(paste("Logistic AUC:", round(logit_auc, 4)),
paste("LDA AUC:", round(lda_auc, 4))),
col = c("blue", "red"), lwd = 2)
#vif_values <- car::vif(lm_model)
#print(vif_values)
#LDA
# Ensure clean data for both models
df_clean <- df2[complete.cases(df2), ]
# Train logistic model (already done earlier, so skip if done)
# logit_model <- glm(y ~ ., data = df_clean, family = binomial)
# Train LDA model
library(MASS)
lda_model <- lda(y ~ ., data = df_clean)
summary(lda_model)
# Predict probabilities
logit_probs <- predict(logit_model, newdata = df_clean, type = "response")
logit_pred <- prediction(logit_probs, df_clean$y)
logit_perf <- performance(logit_pred, "tpr", "fpr")
# Plot ROC
plot(logit_perf, col = "blue", lwd = 2,
main = "ROC Curve: Logistic Regression")
abline(a = 0, b = 1, lty = 2, col = "gray")
# AUC
logit_auc <- performance(logit_pred, "auc")@y.values[[1]]
print(paste("Logistic Regression AUC:", round(logit_auc, 4)))
# Predict probabilities from LDA
lda_pred <- predict(lda_model, newdata = df_clean)
lda_probs <- lda_pred$posterior[, 2]
# ROC and AUC for LDA
lda_roc <- prediction(lda_probs, df_clean$y)
lda_perf <- performance(lda_roc, "tpr", "fpr")
# Plot ROC
plot(lda_perf, col = "red", lwd = 2,
main = "ROC Curve: Linear Discriminant Analysis")
abline(a = 0, b = 1, lty = 2, col = "gray")
# AUC
lda_auc <- performance(lda_roc, "auc")@y.values[[1]]
print(paste("LDA AUC:", round(lda_auc, 4)))
### Model Performance Summary
| Model                | AUC     |
### Model Performance Summary
| Model                | AUC     |
# Create a data frame with model performance
model_summary <- data.frame(
Model = c("Logistic Regression", "LDA"),
AUC = c(round(logit_auc, 4), round(lda_auc, 4))
)
# Display the table nicely
knitr::kable(model_summary, caption = "Model Performance Summary (AUC)")
# Confusion Matrix for Logistic Regression
logit_class <- ifelse(logit_probs > 0.5, "yes", "no")
confusion_matrix_logit <- table(Predicted = logit_class, Actual = df_clean$y)
print("Confusion Matrix - Logistic Regression")
# Confusion Matrix for Logistic Regression
logit_class <- ifelse(logit_probs > 0.5, "yes", "no")
confusion_matrix_logit <- table(Predicted = logit_class, Actual = df_clean$y)
print("Confusion Matrix - Logistic Regression")
# Logistic predicted classes
logit_pred_class <- ifelse(predict(logit_model, newdata = df_clean, type = "response") > 0.5, "yes", "no")
# LDA predicted classes
lda_pred_class <- predict(lda_model, newdata = df_clean)$class
# Load required package
library(caret)
# Convert actual target
actual <- df_clean$y
# Confusion matrices
logit_cm <- confusionMatrix(factor(logit_pred_class, levels = c("no", "yes")), actual, positive = "yes")
lda_cm   <- confusionMatrix(factor(lda_pred_class, levels = c("no", "yes")), actual, positive = "yes")
# Load required package
library(caret)
# Convert actual target
actual <- df_clean$y
# Confusion matrices
logit_cm <- confusionMatrix(factor(logit_pred_class, levels = c("no", "yes")), actual, positive = "yes")
lda_cm   <- confusionMatrix(factor(lda_pred_class, levels = c("no", "yes")), actual, positive = "yes")
print(logit_cm)
# Get metrics from confusion matrix
logit_metrics <- logit_cm$byClass
lda_metrics   <- lda_cm$byClass
# Combine into table
model_metrics <- data.frame(
Model       = c("Logistic Regression", "LDA"),
Accuracy    = c(logit_cm$overall["Accuracy"], lda_cm$overall["Accuracy"]),
Sensitivity = c(logit_metrics["Sensitivity"], lda_metrics["Sensitivity"]),
Specificity = c(logit_metrics["Specificity"], lda_metrics["Specificity"]),
AUC         = c(round(logit_auc, 4), round(lda_auc, 4))
)
# Display the table nicely
# Get metrics from confusion matrix
logit_metrics <- logit_cm$byClass
lda_metrics   <- lda_cm$byClass
# Combine into table
model_metrics <- data.frame(
Model       = c("Logistic Regression", "LDA"),
Accuracy    = c(logit_cm$overall["Accuracy"], lda_cm$overall["Accuracy"]),
Sensitivity = c(logit_metrics["Sensitivity"], lda_metrics["Sensitivity"]),
Specificity = c(logit_metrics["Specificity"], lda_metrics["Specificity"]),
AUC         = c(round(logit_auc, 4), round(lda_auc, 4))
)
# Display the table nicely
knitr::kable(model_metrics, caption = "Model Performance Metrics")
# Print the metrics table
knitr::kable(model_metrics, digits = 4, caption = "Model Evaluation Metrics")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(car)
library(GGally)
library(ROCR)
library(data.table)
library(Hmisc)
library(tibble)
library(knitr)
library(here)
library(ROCR)
data_path <- "https://raw.githubusercontent.com/kccasey1985/Case-Study-Bank-Marketting-/main/bank-additional-full.csv"
df <- fread(data_path, sep = ";") |> as_tibble()
glimpse(df)
opts_knit$set(root.dir = here())
# load the dataset
df <- fread(here("bank-additional-full.csv"), sep = ";") |> as_tibble()
glimpse(df)
head(df)
str(df)
summary(df)
df2 <- df |>
mutate(job = factor(job),
marital  = factor(marital),
education  = factor(education),
default  = factor(default),
loan  = factor(loan),
contact  = factor(contact),
month   = factor(month),
day_of_week  = factor(day_of_week),
poutcome    = factor(poutcome),
y   = factor(y),
housing  = factor(housing))
str(df2)
summary(df2)
anyNA(df2)
df2 <- df2 |>
mutate(age= cut(age, breaks = c(20, 30, 40, 50, 60, 70, 100),
labels = c("21-30", "31-40", "41-50", "51-60", "61-70", "71+"),
right = F))|>
select(-duration)
summary(df2)
df2_num <- dplyr::select_if(df2, is.numeric)
matrix = cor(df2_num)
corrplot(matrix, method = c("number"))
corrplot::corrplot(matrix, method = "circle")
lm_model <- lm(y ~ ., data = df2)
# Make sure y is a factor with two levels
df2$y <- factor(df2$y, levels = c("no", "yes"))
# Logistic regression
logit_model <- glm(y ~ ., data = df2, family = binomial)
summary(logit_model)
alias(lm_model)
#vif_values <- car::vif(lm_model)
#print(vif_values)
#LDA
# Ensure clean data for both models
df_clean <- df2[complete.cases(df2), ]
# Train logistic model (already done earlier, so skip if done)
# logit_model <- glm(y ~ ., data = df_clean, family = binomial)
# Train LDA model
library(MASS)
lda_model <- lda(y ~ ., data = df_clean)
summary(lda_model)
# Predict probabilities
logit_probs <- predict(logit_model, newdata = df_clean, type = "response")
logit_pred <- prediction(logit_probs, df_clean$y)
logit_perf <- performance(logit_pred, "tpr", "fpr")
# Plot ROC
plot(logit_perf, col = "blue", lwd = 2,
main = "ROC Curve: Logistic Regression")
abline(a = 0, b = 1, lty = 2, col = "gray")
# AUC
logit_auc <- performance(logit_pred, "auc")@y.values[[1]]
print(paste("Logistic Regression AUC:", round(logit_auc, 4)))
# Predict probabilities from LDA
lda_pred <- predict(lda_model, newdata = df_clean)
lda_probs <- lda_pred$posterior[, 2]
# ROC and AUC for LDA
lda_roc <- prediction(lda_probs, df_clean$y)
lda_perf <- performance(lda_roc, "tpr", "fpr")
# Plot ROC
plot(lda_perf, col = "red", lwd = 2,
main = "ROC Curve: Linear Discriminant Analysis")
abline(a = 0, b = 1, lty = 2, col = "gray")
# AUC
lda_auc <- performance(lda_roc, "auc")@y.values[[1]]
print(paste("LDA AUC:", round(lda_auc, 4)))
# Create a data frame with model performance
model_summary <- data.frame(
Model = c("Logistic Regression", "LDA"),
AUC = c(round(logit_auc, 4), round(lda_auc, 4))
)
# Display the table nicely
knitr::kable(model_summary, caption = "Model Performance Summary (AUC)")
# Logistic predicted classes
logit_pred_class <- ifelse(predict(logit_model, newdata = df_clean, type = "response") > 0.5, "yes", "no")
# LDA predicted classes
lda_pred_class <- predict(lda_model, newdata = df_clean)$class
# Load required package
library(caret)
# Convert actual target
actual <- df_clean$y
# Confusion matrices
logit_cm <- confusionMatrix(factor(logit_pred_class, levels = c("no", "yes")), actual, positive = "yes")
lda_cm   <- confusionMatrix(factor(lda_pred_class, levels = c("no", "yes")), actual, positive = "yes")
print(logit_cm)
# Get metrics from confusion matrix
logit_metrics <- logit_cm$byClass
lda_metrics   <- lda_cm$byClass
# Combine into table
model_metrics <- data.frame(
Model       = c("Logistic Regression", "LDA"),
Accuracy    = c(logit_cm$overall["Accuracy"], lda_cm$overall["Accuracy"]),
Sensitivity = c(logit_metrics["Sensitivity"], lda_metrics["Sensitivity"]),
Specificity = c(logit_metrics["Specificity"], lda_metrics["Specificity"]),
AUC         = c(round(logit_auc, 4), round(lda_auc, 4))
)
# Display the table nicely
knitr::kable(model_metrics, caption = "Model Performance Metrics")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(car)
library(GGally)
library(ROCR)
library(data.table)
library(Hmisc)
library(tibble)
library(knitr)
library(here)
library(ROCR)
data_path <- "https://raw.githubusercontent.com/kccasey1985/Case-Study-Bank-Marketting-/main/bank-additional-full.csv"
df <- fread(data_path, sep = ";") |> as_tibble()
glimpse(df)
opts_knit$set(root.dir = here())
# load the dataset
df <- fread(here("bank-additional-full.csv"), sep = ";") |> as_tibble()
glimpse(df)
head(df)
str(df)
summary(df)
df2 <- df |>
mutate(job = factor(job),
marital  = factor(marital),
education  = factor(education),
default  = factor(default),
loan  = factor(loan),
contact  = factor(contact),
month   = factor(month),
day_of_week  = factor(day_of_week),
poutcome    = factor(poutcome),
y   = factor(y),
housing  = factor(housing))
str(df2)
summary(df2)
anyNA(df2)
df2 <- df2 |> mutate(age= cut(age, breaks = c(20, 30, 40, 50, 60, 70, 100),
labels = c("21-30", "31-40", "41-50", "51-60", "61-70", "71+"),
right = F))|>
select(-duration)
summary(df2)
df2_num <- dplyr::select_if(df2, is.numeric)
matrix = cor(df2_num)
corrplot(matrix, method = c("number"))
corrplot::corrplot(matrix, method = "circle")
lm_model <- lm(y ~ ., data = df2)
# Make sure y is a factor with two levels
df2$y <- factor(df2$y, levels = c("no", "yes"))
# Logistic regression
logit_model <- glm(y ~ ., data = df2, family = binomial)
summary(logit_model)
alias(lm_model)
vif_values <- car::vif(lm_model)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(car)
library(GGally)
library(ROCR)
library(data.table)
library(Hmisc)
library(tibble)
library(knitr)
library(here)
library(ROCR)
data_path <- "https://raw.githubusercontent.com/kccasey1985/Case-Study-Bank-Marketting-/main/bank-additional-full.csv"
df <- fread(data_path, sep = ";") |> as_tibble()
glimpse(df)
opts_knit$set(root.dir = here())
# load the dataset
df <- fread(here("bank-additional-full.csv"), sep = ";") |> as_tibble()
glimpse(df)
head(df)
str(df)
summary(df)
df2 <- df |>
mutate(job = factor(job),
marital  = factor(marital),
education  = factor(education),
default  = factor(default),
loan  = factor(loan),
contact  = factor(contact),
month   = factor(month),
day_of_week  = factor(day_of_week),
poutcome    = factor(poutcome),
y   = factor(y),
housing  = factor(housing))
str(df2)
summary(df2)
anyNA(df2)
df2 <- df2 |> mutate(age= cut(age, breaks = c(20, 30, 40, 50, 60, 70, 100),
labels = c("21-30", "31-40", "41-50", "51-60", "61-70", "71+"),
right = F))|>
select(-duration)
summary(df2)
df2_num <- dplyr::select_if(df2, is.numeric)
matrix = cor(df2_num)
corrplot(matrix, method = c("number"))
corrplot::corrplot(matrix, method = "circle")
lm_model <- lm(y ~ ., data = df2)
# Make sure y is a factor with two levels
df2$y <- factor(df2$y, levels = c("no", "yes"))
# Logistic regression
logit_model <- glm(y ~ ., data = df2, family = binomial)
summary(logit_model)
alias(lm_model)
#vif_values <- car::vif(lm_model)
#
print(vif_values)
#remove factors with greater than 0.875 correlation with other factors
df3 <- subset(df2, select = -c(emp.var.rate,cons.price.idx,euribor3m,nr.employed))
df3
#redo correction matrix with removed factors with greater than 0.875 correlation with other factors
df3_num <- dplyr::select_if(df3, is.numeric)
matrix2 = cor(df3_num)
corrplot(matrix2, method = c("number"))
# Logistic regression with df3
lm_model_2 <- lm(y ~ . , data = df3)
logit_model2 <- glm(y ~ ., data = df3, family = binomial)
summary(logit_model2)
# Make sure y is a factor with two levels
df3$y <- factor(df3$y, levels = c("no", "yes"))
# Logistic regression
logit_model <- glm(y ~ ., data = df3, family = binomial)
summary(logit_model)
